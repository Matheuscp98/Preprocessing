{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ab4ca-728d-4a0c-8ebe-08a7f9781b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "# Imports and global config\n",
    "from __future__ import annotations\n",
    "\n",
    "# Standard library\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import time\n",
    "import warnings\n",
    "import re\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "# Core data science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pymoo.visualization.scatter import Scatter\n",
    "\n",
    "# Statistics and Factor Analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import kruskal, mannwhitneyu, shapiro, levene\n",
    "from scipy.linalg import cholesky, det, eigvals, inv\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "\n",
    "# Decomposition and Discriminant Analysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Ensembles and Tree-based Methods\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "# Feature Selection and Importance\n",
    "from boruta import BorutaPy\n",
    "from sklearn.feature_selection import RFE, f_classif, mutual_info_classif\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Linear/Logistic Regression and Classification\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "# Model Evaluation and Cross-Validation\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import (\n",
    "    KFold, LeaveOneOut, StratifiedKFold,\n",
    "    cross_val_predict, cross_validate, cross_val_score,\n",
    "    GridSearchCV, learning_curve, train_test_split\n",
    ")\n",
    "\n",
    "# Probabilistic and Distance-based Methods\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Neural Networks and Pipelines\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Boosting Libraries\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Genetic Algorithms and Bayesian Optimization\n",
    "from deap import base, creator, tools, algorithms\n",
    "import optuna\n",
    "from optuna.exceptions import ExperimentalWarning\n",
    "\n",
    "# Explainability/Model Interpretation\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import shap\n",
    "\n",
    "# Design of Experiments (DOE)\n",
    "from pyDOE2 import fracfact\n",
    "\n",
    "# General: suppress non-critical warnings\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=ExperimentalWarning)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71873d65-cde2-4892-8131-ee0cb651a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading\n",
    "\n",
    "\n",
    "# List of dataset base filenames (without extension)\n",
    "dataset_names = [\n",
    "    \"statlog_australian_credit_approval\",\n",
    "    \"wine_quality_red\",\n",
    "    \"diabetic_retinopathy_debrecen\",\n",
    "    \"breast_cancer_wisconsin\",\n",
    "    \"burst_header_packet\",\n",
    "    \"seismic_bumps\",\n",
    "    \"iranian_churn\",\n",
    "    \"qsar_biodegradation\",\n",
    "    \"image_segmentation\",\n",
    "    \"wine_quality_white\",\n",
    "    \"steel_plates_faults\",\n",
    "    \"gender_gap_spanish_wp\",\n",
    "    \"waveform_database_generator\"\n",
    "]\n",
    "\n",
    "# Dictionary to hold each loaded DataFrame keyed by dataset name\n",
    "datasets = {}\n",
    "\n",
    "for name in dataset_names:\n",
    "    # Load each dataset from CSV into a DataFrame\n",
    "    df = pd.read_csv(f\"datasets/{name}.csv\")\n",
    "\n",
    "    # Drop any rows containing at least one missing value\n",
    "    # (ensures consistent model input across datasets)\n",
    "    df = df.dropna(axis=0, how=\"any\")\n",
    "\n",
    "    # Store the cleaned DataFrame\n",
    "    datasets[name] = df\n",
    "\n",
    "# Container for per-dataset outputs/results (e.g., metrics, models, artifacts)\n",
    "outputs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2ed2c9-4fb8-48e9-857f-737368fd7c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data bases (Original, PCA, and PCFA)\n",
    "\n",
    "\n",
    "# Iterate over all datasets\n",
    "for i, (name, original_df) in enumerate(datasets.items(), start=1):\n",
    "    # Remove rows containing any NaNs and reset index\n",
    "    original_df = original_df.dropna().reset_index(drop=True)\n",
    "\n",
    "    # Remove constant columns to avoid degenerate components (inf/NaN in decompositions)\n",
    "    X = original_df.drop(columns=\"y\")\n",
    "    X = X.loc[:, X.nunique() > 1]\n",
    "    y = original_df[\"y\"].values\n",
    "\n",
    "    # Standardize features (mean=0, std=1) for PCA/PCFA\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # PCA to determine the number of components (retain >= 85% cumulative variance)\n",
    "    pca = PCA(n_components=0.85, svd_solver=\"full\")\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    n_components_pca = X_pca.shape[1]\n",
    "\n",
    "    # PCA scores DataFrame\n",
    "    pca_df = pd.DataFrame(\n",
    "        X_pca,\n",
    "        columns=[f\"PC{j+1}\" for j in range(n_components_pca)],\n",
    "        index=original_df.index\n",
    "    )\n",
    "    pca_df[\"y\"] = y\n",
    "\n",
    "    # PCFA (PCA-informed Factor Analysis) with Varimax rotation\n",
    "    # Start with at least 2 factors, using the PCA-derived count as baseline\n",
    "    n_components_fa = max(n_components_pca, 2)\n",
    "    fa = FactorAnalyzer(\n",
    "        n_factors=n_components_fa,\n",
    "        rotation=\"varimax\",\n",
    "        method=\"principal\"\n",
    "    )\n",
    "    try:\n",
    "        X_factors = fa.fit_transform(X_scaled)\n",
    "    except Exception:\n",
    "        # Fallback: if FA fails (e.g., due to rank deficiency), reduce number of factors\n",
    "        n_components_fa = min(n_components_fa, X_scaled.shape[1] - 1)\n",
    "        fa = FactorAnalyzer(\n",
    "            n_factors=n_components_fa,\n",
    "            rotation=\"varimax\",\n",
    "            method=\"principal\"\n",
    "        )\n",
    "        X_factors = fa.fit_transform(X_scaled)\n",
    "\n",
    "    # PCFA scores DataFrame\n",
    "    fa_df = pd.DataFrame(\n",
    "        X_factors,\n",
    "        columns=[f\"F{j+1}\" for j in range(n_components_fa)],\n",
    "        index=original_df.index\n",
    "    )\n",
    "    fa_df[\"y\"] = y\n",
    "\n",
    "    # Store outputs for this dataset\n",
    "    ds_key = f\"dataset_{i}\"\n",
    "    outputs.setdefault(ds_key, {})\n",
    "    outputs[ds_key][\"original\"]   = original_df\n",
    "    outputs[ds_key][\"pca_scores\"] = pca_df\n",
    "    outputs[ds_key][\"fa_scores\"]  = fa_df\n",
    "\n",
    "\n",
    "# Persist transformed datasets to Excel files\n",
    "\n",
    "# Keep sheet names within Excel's 31-character limit\n",
    "enums = list(datasets.keys())\n",
    "\n",
    "# Original\n",
    "with pd.ExcelWriter(\"df_original.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    for i, name in enumerate(enums, start=1):\n",
    "        df_orig = outputs[f\"dataset_{i}\"][\"original\"]\n",
    "        sheet = f\"{i}_{name}\"[:31]\n",
    "        df_orig.to_excel(writer, sheet_name=sheet, index=False)\n",
    "\n",
    "# PCA\n",
    "with pd.ExcelWriter(\"df_pca.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    for i, name in enumerate(enums, start=1):\n",
    "        df_pca = outputs[f\"dataset_{i}\"][\"pca_scores\"]\n",
    "        sheet = f\"{i}_{name}\"[:31]\n",
    "        df_pca.to_excel(writer, sheet_name=sheet, index=False)\n",
    "\n",
    "# PCFA (FA)\n",
    "with pd.ExcelWriter(\"df_fa.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    for i, name in enumerate(enums, start=1):\n",
    "        df_fa = outputs[f\"dataset_{i}\"][\"fa_scores\"]\n",
    "        sheet = f\"{i}_{name}\"[:31]\n",
    "        df_fa.to_excel(writer, sheet_name=sheet, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41af86f-474a-457e-ae32-cd04641a42ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset selection and factorial design\n",
    "\n",
    "# Config\n",
    "DATASET  = 'original'  # options: 'original', 'pca', 'fa'\n",
    "seed      = 42\n",
    "fraction  = 1.0        # 1.0 = full factorial before hitting max_iter\n",
    "max_iter  = 256        # desired maximum number of iterations (rows) per design\n",
    "\n",
    "def create_full_and_frac_designs(\n",
    "    num_features: int,\n",
    "    fraction: float = 1.0,\n",
    "    seed: int | None = None,\n",
    "    prefix: str | list[str] = 'F',\n",
    "    max_iter: int | None = None\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Create full and fractional factorial designs with two levels (-1, +1).\n",
    "\n",
    "    If the full design (2**num_features) would exceed max_iter, generate a\n",
    "    random fractional sample directly with size = max_iter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_features : int\n",
    "        Number of factors (features) to include in the design.\n",
    "    fraction : float, default=1.0\n",
    "        Fraction of the full design to keep in the fractional design (0 < fraction <= 1).\n",
    "    seed : int | None\n",
    "        Random seed for reproducibility.\n",
    "    prefix : str | list[str], default='F'\n",
    "        Column names. If str, columns will be prefix + index (e.g., F1, F2, ...).\n",
    "        If list, it is used as-is.\n",
    "    max_iter : int | None\n",
    "        Cap the number of design rows. If None, no cap is applied.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    full_df : pd.DataFrame\n",
    "        Full factorial design (possibly capped by max_iter).\n",
    "    frac_df : pd.DataFrame\n",
    "        Fractional design (random subset) honoring `fraction` and `max_iter`.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Build column names\n",
    "    if isinstance(prefix, list):\n",
    "        cols = prefix\n",
    "    else:\n",
    "        cols = [f\"{prefix}{i+1}\" for i in range(num_features)]\n",
    "\n",
    "    total_pts = 2 ** num_features\n",
    "\n",
    "    # If a full design would exceed max_iter, create a random fractional sample directly\n",
    "    if max_iter is not None and total_pts > max_iter:\n",
    "        n_sel = max_iter\n",
    "        mat = rng.choice([-1, 1], size=(n_sel, num_features))\n",
    "        full_df = pd.DataFrame(mat, columns=cols)\n",
    "        full_df.insert(0, \"Iteration\", np.arange(1, n_sel + 1))\n",
    "        frac_df = full_df.copy()\n",
    "    else:\n",
    "        # Full factorial matrix\n",
    "        mat = np.array(np.meshgrid(*([[-1, 1]] * num_features))).T.reshape(-1, num_features)\n",
    "        full_df = pd.DataFrame(mat, columns=cols)\n",
    "        full_df.insert(0, \"Iteration\", np.arange(1, len(full_df) + 1))\n",
    "\n",
    "        # Optionally cap the full design to max_iter\n",
    "        if max_iter is not None and len(full_df) > max_iter:\n",
    "            sel = rng.choice(full_df.index, size=max_iter, replace=False)\n",
    "            full_df = full_df.loc[sel].reset_index(drop=True)\n",
    "            full_df[\"Iteration\"] = np.arange(1, len(full_df) + 1)\n",
    "\n",
    "        # Fractional design (random subset of the full design)\n",
    "        if 0 < fraction < 1.0:\n",
    "            n_sel = max(1, int(len(full_df) * fraction))\n",
    "            if max_iter is not None:\n",
    "                n_sel = min(n_sel, max_iter)\n",
    "            idxs = rng.choice(full_df.index, size=n_sel, replace=False)\n",
    "            frac_df = full_df.loc[idxs].reset_index(drop=True)\n",
    "            frac_df[\"Iteration\"] = np.arange(1, len(frac_df) + 1)\n",
    "        else:\n",
    "            frac_df = full_df.copy()\n",
    "\n",
    "    return full_df, frac_df\n",
    "\n",
    "# Iterate through each loaded dataset\n",
    "for i, (name, _) in enumerate(datasets.items(), start=1):\n",
    "    ds_key = f\"dataset_{i}\"\n",
    "    orig   = outputs[ds_key][\"original\"]\n",
    "    pca    = outputs[ds_key][\"pca_scores\"]\n",
    "    fa_df  = outputs[ds_key][\"fa_scores\"]\n",
    "\n",
    "    # Select the working DataFrame and set column prefixes accordingly\n",
    "    if DATASET == 'original':\n",
    "        df_used = orig\n",
    "        X_full  = df_used.drop(columns='y')\n",
    "        prefix  = list(X_full.columns)\n",
    "    elif DATASET == 'pca':\n",
    "        df_used = pca\n",
    "        X_full  = df_used.drop(columns='y')\n",
    "        prefix  = [f\"PC{j+1}\" for j in range(X_full.shape[1])]\n",
    "    else:\n",
    "        df_used = fa_df\n",
    "        X_full  = df_used.drop(columns='y')\n",
    "        prefix  = [f\"F{j+1}\" for j in range(X_full.shape[1])]\n",
    "\n",
    "    y_full       = df_used['y']\n",
    "    num_features = X_full.shape[1]\n",
    "\n",
    "    # Create full and fractional designs honoring max_iter\n",
    "    full_df, frac_df = create_full_and_frac_designs(\n",
    "        num_features, fraction, seed,\n",
    "        prefix=prefix, max_iter=max_iter\n",
    "    )\n",
    "\n",
    "    outputs[ds_key][\"design\"] = frac_df\n",
    "\n",
    "# Persist designs to an Excel workbook (one sheet per dataset)\n",
    "with pd.ExcelWriter(\"df_designs.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    for i, (name, _) in enumerate(datasets.items(), start=1):\n",
    "        df_design = outputs[f\"dataset_{i}\"][\"design\"]\n",
    "        sheet     = f\"{i}_{name}_{DATASET}\"[:31]  # Excel sheet name limit is 31 chars\n",
    "        df_design.to_excel(writer, sheet_name=sheet, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8905b6fa-33ce-49db-b0a8-b78b00c5dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML models: training & evaluation\n",
    "\n",
    "\n",
    "# Iterate over each dataset in `datasets`\n",
    "for i, (name, _) in enumerate(datasets.items(), start=1):\n",
    "    ds_key = f\"dataset_{i}\"\n",
    "    \n",
    "    # Select the input DataFrame according to DATASET flag\n",
    "    df_used = {\n",
    "        'original': outputs[ds_key][\"original\"],\n",
    "        'pca'     : outputs[ds_key][\"pca_scores\"],\n",
    "        'fa'      : outputs[ds_key][\"fa_scores\"]\n",
    "    }[DATASET]\n",
    "    \n",
    "    # Prepare X, y and the factorial design\n",
    "    X_full    = df_used.drop(columns=\"y\")\n",
    "    y_full    = df_used[\"y\"]\n",
    "    y         = LabelEncoder().fit_transform(y_full)\n",
    "    X         = X_full.copy()\n",
    "    design_df = outputs[ds_key][\"design\"]\n",
    "    \n",
    "    # Define models and scoring metrics\n",
    "    MODELS = {\n",
    "        'KNN':  KNeighborsClassifier(n_jobs=-1),\n",
    "        'LR':   LogisticRegression(max_iter=1000, n_jobs=-1),\n",
    "        'LDA':  LinearDiscriminantAnalysis(),\n",
    "        'NB':   GaussianNB(),\n",
    "        'SVM':  SVC(random_state=42),\n",
    "        'RF':   RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        'GB':   GradientBoostingClassifier(random_state=42),\n",
    "        'XGB':  XGBClassifier(use_label_encoder=False, random_state=42, n_jobs=-1),\n",
    "        'LGBM': LGBMClassifier(random_state=42, n_jobs=-1, verbose=-1),\n",
    "        'MLP':  MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10)\n",
    "    }\n",
    "    SCORING = {\n",
    "        'A' : 'accuracy',\n",
    "        'P' : 'precision_weighted',\n",
    "        'R' : 'recall_weighted',\n",
    "        'F1': 'f1_weighted'\n",
    "    }\n",
    "\n",
    "    all_results = []\n",
    "    all_times   = []\n",
    "\n",
    "    # Evaluate each design iteration (row) produced by DOE\n",
    "    for idx, design_row in design_df.iterrows():\n",
    "        # Select columns flagged with +1 in the design (and that exist in X)\n",
    "        sel_cols    = [c for c, v in design_row.items() if v == 1 and c in X.columns]\n",
    "        perf_scores = {}\n",
    "        time_scores = {}\n",
    "\n",
    "        if not sel_cols:\n",
    "            # No features selected: fill with NaNs for metrics and time\n",
    "            for m in MODELS:\n",
    "                for abbr in SCORING:\n",
    "                    perf_scores[f\"{m}({abbr})\"] = np.nan\n",
    "                time_scores[f\"{m}_time\"] = np.nan\n",
    "        else:\n",
    "            X_sub = X[sel_cols]\n",
    "            cv    = StratifiedKFold(n_splits=5, shuffle=True, random_state=idx)\n",
    "\n",
    "            # Cross-validate each model on the selected subset of features\n",
    "            for mname, model in MODELS.items():\n",
    "                start  = time.time()\n",
    "                cv_res = cross_validate(\n",
    "                    model,\n",
    "                    X_sub, y,\n",
    "                    cv=cv,\n",
    "                    scoring=SCORING,\n",
    "                    n_jobs=-1,\n",
    "                    return_train_score=False\n",
    "                )\n",
    "                elapsed = time.time() - start\n",
    "\n",
    "                # Store mean test scores for each metric\n",
    "                for abbr in SCORING:\n",
    "                    perf_scores[f\"{mname}({abbr})\"] = cv_res[f\"test_{abbr}\"].mean()\n",
    "                time_scores[f\"{mname}_time\"] = elapsed\n",
    "\n",
    "        all_results.append(perf_scores)\n",
    "        all_times.append(time_scores)\n",
    "\n",
    "    # Build results/time DataFrames for this dataset\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    times_df   = pd.DataFrame(all_times)\n",
    "\n",
    "    # Enforce a consistent column order\n",
    "    metric_cols = [f\"{m}({abbr})\" for m in MODELS for abbr in SCORING]\n",
    "    time_cols   = [f\"{m}_time\"   for m in MODELS]\n",
    "    results_df  = results_df[metric_cols]\n",
    "    times_df    = times_df[time_cols]\n",
    "\n",
    "    # Store in outputs\n",
    "    outputs[ds_key][\"results\"] = results_df\n",
    "    outputs[ds_key][\"times\"]   = times_df\n",
    "\n",
    "# Persist per-design results to Excel workbooks\n",
    "with pd.ExcelWriter(\"df_ml_results.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    for i, (name, _) in enumerate(datasets.items(), start=1):\n",
    "        df_res = outputs[f\"dataset_{i}\"][\"results\"]\n",
    "        sheet  = f\"{i}_{name}_{DATASET}\"[:31]  # Excel sheet name limit is 31 characters\n",
    "        df_res.to_excel(writer, sheet_name=sheet, index=False)\n",
    "\n",
    "with pd.ExcelWriter(\"df_ml_times.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    for i, (name, _) in enumerate(datasets.items(), start=1):\n",
    "        df_time = outputs[f\"dataset_{i}\"][\"times\"]\n",
    "        sheet   = f\"{i}_{name}_{DATASET}\"[:31]\n",
    "        df_time.to_excel(writer, sheet_name=sheet, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a925803-30da-426c-ba05-def71bb3b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics computation\n",
    "\n",
    "\n",
    "# Iterate over each dataset to compute statistics\n",
    "for i, (name, _) in enumerate(datasets.items(), start=1):\n",
    "    ds_key      = f\"dataset_{i}\"\n",
    "    # Ensure numeric conversion; invalid values become NaN\n",
    "    results_df  = outputs[ds_key][\"results\"].copy().apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Detailed descriptive statistics for each column (Model(Metric))\n",
    "    stat_names = ['Mean', 'Median', 'Range', 'Max', 'Min', 'Std Dev']\n",
    "    descriptive_stats_df = pd.DataFrame({\n",
    "        'Mean'   : results_df.mean(),\n",
    "        'Median' : results_df.median(),\n",
    "        'Range'  : results_df.max() - results_df.min(),\n",
    "        'Max'    : results_df.max(),\n",
    "        'Min'    : results_df.min(),\n",
    "        'Std Dev': results_df.std()\n",
    "    })[stat_names]\n",
    "    \n",
    "    # Aggregated statistics per model (averaging A, P, R, F1 across iterations)\n",
    "    metrics_to_average = ['A','P','R','F1']\n",
    "    model_prefixes = sorted({col.split('(')[0] for col in results_df.columns})\n",
    "    agg_stats = {}\n",
    "    for prefix in model_prefixes:\n",
    "        # Collect only existing metric columns for the current model\n",
    "        cols = [f\"{prefix}({m})\" for m in metrics_to_average if f\"{prefix}({m})\" in results_df]\n",
    "        if cols:\n",
    "            avg_per_iter = results_df[cols].mean(axis=1)  # Average across metrics for each iteration\n",
    "            agg_stats[prefix] = [\n",
    "                avg_per_iter.mean(),                      # Mean performance across all iterations\n",
    "                avg_per_iter.median(),                    # Median performance\n",
    "                avg_per_iter.max() - avg_per_iter.min(),  # Range\n",
    "                avg_per_iter.max(),                       # Max performance\n",
    "                avg_per_iter.min(),                       # Min performance\n",
    "                avg_per_iter.std()                        # Standard deviation\n",
    "            ]\n",
    "        else:\n",
    "            agg_stats[prefix] = [np.nan]*len(stat_names)\n",
    "    \n",
    "    # Create a DataFrame for aggregated statistics sorted by Mean in descending order\n",
    "    final_summary_df = (pd.DataFrame.from_dict(\n",
    "        agg_stats, orient='index', columns=stat_names\n",
    "    ).sort_values('Mean', ascending=False).T)\n",
    "    \n",
    "    # Store the computed statistics\n",
    "    outputs[ds_key][\"descriptive_stats\"] = descriptive_stats_df\n",
    "    outputs[ds_key][\"summary_stats\"]     = final_summary_df\n",
    "\n",
    "\n",
    "# Save statistics to Excel files\n",
    "with pd.ExcelWriter(\"df_descriptive_stats.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    for i, (name, _) in enumerate(datasets.items(), start=1):\n",
    "        df_detail = outputs[f\"dataset_{i}\"][\"descriptive_stats\"]\n",
    "        sheet     = f\"{i}_{name}_{DATASET}\"[:31]  # Limit sheet name to Excel's 31-character max\n",
    "        df_detail.to_excel(writer, sheet_name=sheet, index=True)\n",
    "\n",
    "with pd.ExcelWriter(\"df_summary_stats.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    for i, (name, _) in enumerate(datasets.items(), start=1):\n",
    "        df_summary = outputs[f\"dataset_{i}\"][\"summary_stats\"]\n",
    "        sheet      = f\"{i}_{name}_{DATASET}\"[:31]\n",
    "        df_summary.to_excel(writer, sheet_name=sheet, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98daa8fa-accd-4ae4-b530-44a2023f6e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best results selection\n",
    "\n",
    "\n",
    "# Iterate through each dataset to determine the best iterations according to different criteria\n",
    "for i, (name, _) in enumerate(datasets.items(), start=1):\n",
    "    ds_key     = f\"dataset_{i}\"\n",
    "    results_df = outputs[ds_key][\"results\"].copy()\n",
    "    design_df  = outputs[ds_key][\"design\"].copy()\n",
    "    \n",
    "    # Identify performance columns (those with the format Model(Metric))\n",
    "    perf_cols = [c for c in results_df.columns if '(' in c and ')' in c]\n",
    "    perf_data = results_df[perf_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Compute per-iteration statistics\n",
    "    stats = pd.DataFrame({\n",
    "        'Mean'   : perf_data.mean(axis=1),\n",
    "        'Median' : perf_data.median(axis=1),\n",
    "        'Range'  : perf_data.max(axis=1) - perf_data.min(axis=1),\n",
    "        'Max'    : perf_data.max(axis=1),\n",
    "        'Min'    : perf_data.min(axis=1),\n",
    "        'Std Dev': perf_data.std(axis=1)\n",
    "    }, index=perf_data.index)\n",
    "    \n",
    "    # Identify the iteration index for each \"best\" criterion\n",
    "    best_criteria = {\n",
    "        'Max of Mean'    : stats['Mean'].idxmax(),      # Iteration with highest mean score\n",
    "        'Max of Median'  : stats['Median'].idxmax(),    # Iteration with highest median score\n",
    "        'Min of Range'   : stats['Range'].idxmin(),     # Iteration with most consistent results (smallest range)\n",
    "        'Max of Max'     : stats['Max'].idxmax(),       # Iteration with highest single metric\n",
    "        'Max of Min'     : stats['Min'].idxmax(),       # Iteration with best worst-case metric\n",
    "        'Min of Std Dev' : stats['Std Dev'].idxmin()    # Iteration with lowest variability\n",
    "    }\n",
    "    \n",
    "    # Average performance per algorithm for each iteration\n",
    "    algorithm_means_df = perf_data.groupby(lambda c: c.split('(')[0], axis=1).mean()\n",
    "    \n",
    "    # Prepare iteration number, number of features, and feature list string\n",
    "    iter_numbers    = design_df['Iteration']\n",
    "    feature_cols    = [c for c in design_df.columns if c != 'Iteration']\n",
    "    feature_strings = (\n",
    "        design_df[feature_cols]\n",
    "        .astype(int)\n",
    "        .apply(lambda row: ','.join(row.index[row == 1]), axis=1)\n",
    "    )\n",
    "    \n",
    "    # Build final DataFrame with the best results for each criterion\n",
    "    rows = []\n",
    "    for criterion, idx in best_criteria.items():\n",
    "        num_feats = int((design_df.loc[idx, feature_cols] == 1).sum())\n",
    "        row = {\n",
    "            'Criterion'      : criterion,\n",
    "            'Original Index' : idx,\n",
    "            'Iteration'      : iter_numbers.at[idx],\n",
    "            'Num Features'   : num_feats,\n",
    "            'Features'       : feature_strings.at[idx]\n",
    "        }\n",
    "        row.update(algorithm_means_df.loc[idx].to_dict())\n",
    "        rows.append(row)\n",
    "    \n",
    "    best_results_df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Store results in outputs\n",
    "    outputs[ds_key][\"best_results\"] = best_results_df\n",
    "\n",
    "\n",
    "# Save best results to Excel file\n",
    "with pd.ExcelWriter(\"df_best_results.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    for i, (name, _) in enumerate(datasets.items(), start=1):\n",
    "        sheet = f\"{i}_{name}_{DATASET}\"[:31]  # Excel sheet name limit\n",
    "        outputs[f\"dataset_{i}\"][\"best_results\"].to_excel(\n",
    "            writer, sheet_name=sheet, index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303b632-b997-4d6b-b22c-1f53460a8d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best single iteration selection\n",
    "\n",
    "# Iterate over each dataset\n",
    "for i, (name, _) in enumerate(datasets.items(), start=1):\n",
    "    ds_key     = f\"dataset_{i}\"\n",
    "    results_df = outputs[ds_key][\"results\"].copy()\n",
    "    design_df  = outputs[ds_key][\"design\"].copy()\n",
    "    \n",
    "    # Identify performance columns (metrics) and feature selection columns\n",
    "    perf_cols    = [c for c in results_df.columns if '(' in c and ')' in c]\n",
    "    feature_cols = [c for c in design_df.columns if c != 'Iteration']\n",
    "\n",
    "    # Calculate iteration-level statistics based on performance metrics\n",
    "    perf_data = results_df[perf_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    stats = pd.DataFrame({\n",
    "        'Mean'   : perf_data.mean(axis=1),                               # Mean performance across all metrics\n",
    "        'Median' : perf_data.median(axis=1),                             # Median performance across all metrics\n",
    "        'Range'  : perf_data.max(axis=1) - perf_data.min(axis=1),        # Spread between best and worst metric\n",
    "        'Max'    : perf_data.max(axis=1),                                # Best single metric value\n",
    "        'Min'    : perf_data.min(axis=1),                                # Worst single metric value\n",
    "        'Std Dev': perf_data.std(axis=1)                                 # Variability of metrics\n",
    "    }, index=perf_data.index)\n",
    "\n",
    "    # Determine candidate iterations for \"best\" according to multiple criteria\n",
    "    best_criteria = {\n",
    "        'Max of Mean'   : stats['Mean'].idxmax(),       # Iteration with highest mean\n",
    "        'Max of Median' : stats['Median'].idxmax(),     # Iteration with highest median\n",
    "        'Min of Range'  : stats['Range'].idxmin(),      # Iteration with smallest range\n",
    "        'Max of Max'    : stats['Max'].idxmax(),        # Iteration with highest single metric\n",
    "        'Max of Min'    : stats['Min'].idxmax(),        # Iteration with highest worst-case\n",
    "        'Min of Std Dev': stats['Std Dev'].idxmin()     # Iteration with lowest variability\n",
    "    }\n",
    "\n",
    "    # Select and break ties: choose iteration that appears most frequently among criteria\n",
    "    sel_idxs   = [v for v in best_criteria.values() if pd.notna(v)]\n",
    "    counts     = pd.Series(sel_idxs).value_counts()\n",
    "    candidates = counts[counts == counts.max()].index.tolist()\n",
    "    if len(candidates) > 1:\n",
    "        avg_perf   = perf_data.loc[candidates].mean(axis=1)  # Tie-breaker: choose with highest average performance\n",
    "        chosen_idx = avg_perf.idxmax()\n",
    "    else:\n",
    "        chosen_idx = candidates[0]\n",
    "\n",
    "    # Extract details for the chosen iteration\n",
    "    iter_num  = design_df.at[chosen_idx, 'Iteration']\n",
    "    row_feat  = design_df.loc[chosen_idx, feature_cols].astype(int)\n",
    "    features  = ','.join(row_feat.index[row_feat == 1])\n",
    "    num_feats = int((row_feat == 1).sum())\n",
    "    metrics   = perf_data.loc[chosen_idx].astype(float)\n",
    "\n",
    "    # Build final DataFrame containing the best iteration's details and metrics\n",
    "    best_results_df = pd.DataFrame([{\n",
    "        'Original Index': chosen_idx,\n",
    "        'Iteration'     : iter_num,\n",
    "        'Num Features'  : num_feats,\n",
    "        'Features'      : features,\n",
    "        **metrics.to_dict()\n",
    "    }])\n",
    "\n",
    "    # Store in outputs\n",
    "    outputs[ds_key][\"best_iteration\"] = best_results_df\n",
    "\n",
    "\n",
    "# Save each best iteration to an Excel file\n",
    "with pd.ExcelWriter(\"df_best_iterations.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    for i, (name, _) in enumerate(datasets.items(), start=1):\n",
    "        sheet = f\"{i}_{name}_{DATASET}\"[:31]  # Excel sheet name limit\n",
    "        outputs[f\"dataset_{i}\"][\"best_iteration\"] \\\n",
    "            .to_excel(writer, sheet_name=sheet, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55f8eb0-36f7-4615-be59-217990f3c243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization with Optuna\n",
    "\n",
    "# CV and scoring configuration\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "SCORING = {\n",
    "    'A' : 'accuracy',\n",
    "    'P' : 'precision_weighted',\n",
    "    'R' : 'recall_weighted',\n",
    "    'F1': 'f1_weighted'\n",
    "}\n",
    "\n",
    "# Factory that builds an Optuna objective for a given model\n",
    "def make_objective(X_top, y_top, model_name):\n",
    "    def objective(trial):\n",
    "        # Hyperparameter suggestions per model\n",
    "        if model_name == 'KNN':\n",
    "            params = {\n",
    "                'n_neighbors': trial.suggest_int('n_neighbors', 1, 30),\n",
    "                'weights': trial.suggest_categorical('weights', ['uniform','distance'])\n",
    "            }\n",
    "            clf = KNeighborsClassifier(**params, n_jobs=-1)\n",
    "\n",
    "        elif model_name == 'LR':\n",
    "            params = {\n",
    "                'C': trial.suggest_loguniform('C', 1e-3, 1e3),\n",
    "                'penalty': 'l2',\n",
    "                'solver': 'lbfgs'\n",
    "            }\n",
    "            clf = LogisticRegression(**params, max_iter=1000, n_jobs=-1)\n",
    "\n",
    "        elif model_name == 'LDA':\n",
    "            clf = LinearDiscriminantAnalysis(solver='svd')\n",
    "\n",
    "        elif model_name == 'NB':\n",
    "            params = {\n",
    "                'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
    "            }\n",
    "            clf = GaussianNB(**params)\n",
    "\n",
    "        elif model_name == 'SVM':\n",
    "            params = {\n",
    "                'C': trial.suggest_loguniform('C', 1e-3, 1e3),\n",
    "                'gamma': trial.suggest_categorical('gamma', ['scale','auto'])\n",
    "            }\n",
    "            clf = SVC(**params, random_state=42)\n",
    "\n",
    "        elif model_name == 'RF':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                'max_depth': trial.suggest_int('max_depth', 2, 32)\n",
    "            }\n",
    "            clf = RandomForestClassifier(**params, random_state=42, n_jobs=-1)\n",
    "\n",
    "        elif model_name == 'GB':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "                'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1.0),\n",
    "                'max_depth': trial.suggest_int('max_depth', 2, 10)\n",
    "            }\n",
    "            clf = GradientBoostingClassifier(**params, random_state=42)\n",
    "\n",
    "        elif model_name == 'XGB':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "                'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1.0),\n",
    "                'max_depth': trial.suggest_int('max_depth', 2, 16)\n",
    "            }\n",
    "            clf = XGBClassifier(**params, use_label_encoder=False,\n",
    "                                random_state=42, n_jobs=-1)\n",
    "\n",
    "        elif model_name == 'LGBM':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 8, 128),\n",
    "                'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1.0)\n",
    "            }\n",
    "            clf = LGBMClassifier(**params, random_state=42, n_jobs=-1, verbose=-1)\n",
    "\n",
    "        elif model_name == 'MLP':\n",
    "            params = {\n",
    "                'hidden_layer_sizes': trial.suggest_categorical(\n",
    "                    'hidden_layer_sizes', [(50,), (100,), (50, 50)]\n",
    "                ),\n",
    "                'alpha': trial.suggest_loguniform('alpha', 1e-5, 1e-1),\n",
    "                'learning_rate_init': trial.suggest_loguniform('learning_rate_init', 1e-4, 1e-1)\n",
    "            }\n",
    "            clf = MLPClassifier(**params, random_state=42,\n",
    "                                max_iter=500, early_stopping=True, n_iter_no_change=10)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "        # Optimize weighted F1 via cross-validation\n",
    "        scores = cross_validate(\n",
    "            clf, X_top, y_top, cv=cv,\n",
    "            scoring='f1_weighted', n_jobs=-1,\n",
    "            return_train_score=False\n",
    "        )\n",
    "        return scores['test_score'].mean()\n",
    "    return objective\n",
    "\n",
    "# Tuning loop per dataset\n",
    "for i, (name, _) in enumerate(datasets.items(), start=1):\n",
    "    ds_key = f\"dataset_{i}\"\n",
    "    \n",
    "    # Select the proper DataFrame according to previous choice\n",
    "    df_used = {\n",
    "        'original': outputs[ds_key]['original'],\n",
    "        'pca'     : outputs[ds_key]['pca_scores'],\n",
    "        'fa'      : outputs[ds_key]['fa_scores']\n",
    "    }[DATASET]\n",
    "    \n",
    "    design_df = outputs[ds_key]['design']\n",
    "    best_idx  = outputs[ds_key]['best_iteration'].at[0, 'Original Index']\n",
    "    \n",
    "    # Build X_top, y_top based on the best iteration's selected features\n",
    "    feat_cols = [c for c in design_df.columns if c != 'Iteration']\n",
    "    selected  = [c for c in feat_cols if design_df.at[best_idx, c] == 1]\n",
    "    X_top     = df_used.drop(columns='y')[selected]\n",
    "    y_top     = LabelEncoder().fit_transform(df_used['y'])\n",
    "    \n",
    "    # Collect results per model\n",
    "    optuna_results = []\n",
    "    \n",
    "    # For each model, compute before/after tuning metrics and record time\n",
    "    for model_name, model in MODELS.items():\n",
    "        # Before tuning\n",
    "        cvb = cross_validate(model, X_top, y_top, cv=cv,\n",
    "                             scoring=SCORING, n_jobs=-1,\n",
    "                             return_train_score=False)\n",
    "        before = {f\"{m}(Before)\": cvb[f\"test_{m}\"].mean() for m in SCORING}\n",
    "        \n",
    "        # Optuna tuning\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        t0    = time.time()\n",
    "        study.optimize(make_objective(X_top, y_top, model_name), n_trials=5)\n",
    "        elapsed = time.time() - t0\n",
    "        best_params = study.best_params\n",
    "        \n",
    "        # After tuning\n",
    "        tuned = model.set_params(**best_params)\n",
    "        cva   = cross_validate(tuned, X_top, y_top,\n",
    "                               cv=cv, scoring=SCORING,\n",
    "                               n_jobs=-1, return_train_score=False)\n",
    "        after = {f\"{m}(After)\": cva[f\"test_{m}\"].mean() for m in SCORING}\n",
    "        \n",
    "        optuna_results.append({\n",
    "            'Model'          : model_name,\n",
    "            **before,\n",
    "            **after,\n",
    "            'Best Parameters': best_params,\n",
    "            'Time (s)'       : elapsed\n",
    "        })\n",
    "    \n",
    "    # Persist per-dataset Optuna results\n",
    "    df_tune = pd.DataFrame(optuna_results).set_index('Model')\n",
    "    outputs[ds_key]['optuna_results'] = df_tune\n",
    "\n",
    "# Save Optuna tuning results to Excel\n",
    "with pd.ExcelWriter(\"df_optuna.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    for i, (name, _) in enumerate(datasets.items(), start=1):\n",
    "        sheet = f\"{i}_{name}_{DATASET}\"[:31]\n",
    "        outputs[f\"dataset_{i}\"]['optuna_results'].to_excel(\n",
    "            writer, sheet_name=sheet, index=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bff379-a729-48f3-8e84-45b593b2fc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization: Grid Search\n",
    "\n",
    "\n",
    "# CV configuration\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Parameter grids per model\n",
    "PARAM_GRIDS = {\n",
    "    'KNN':  {'n_neighbors': [3, 5, 7],            'weights': ['uniform', 'distance']},\n",
    "    'LR':   {'C': [0.01, 0.1, 1, 10]},\n",
    "    'LDA':  {'solver': ['svd']},\n",
    "    'NB':   {'var_smoothing': [1e-12, 1e-10, 1e-8]},\n",
    "    'SVM':  {'C': [0.1, 1, 10],                    'gamma': ['scale', 'auto']},\n",
    "    'RF':   {'n_estimators': [100, 200],           'max_depth': [None, 5, 10]},\n",
    "    'GB':   {'n_estimators': [100, 200],           'learning_rate': [0.01, 0.1], 'max_depth': [3, 5]},\n",
    "    'XGB':  {'n_estimators': [100, 200],           'learning_rate': [0.01, 0.1], 'max_depth': [3, 5]},\n",
    "    'LGBM': {'n_estimators': [100, 200],           'num_leaves': [31, 64], 'learning_rate': [0.01, 0.1]},\n",
    "    'MLP':  {'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "             'alpha': [1e-4, 1e-3, 1e-2],\n",
    "             'learning_rate_init': [1e-3, 1e-2, 1e-1]}\n",
    "}\n",
    "\n",
    "# Grid Search per dataset\n",
    "for i, (name, _) in enumerate(datasets.items(), start=1):\n",
    "    ds_key = f\"dataset_{i}\"\n",
    "    \n",
    "    # Use the same data view chosen previously\n",
    "    df_used = {\n",
    "        'original': outputs[ds_key]['original'],\n",
    "        'pca'     : outputs[ds_key]['pca_scores'],\n",
    "        'fa'      : outputs[ds_key]['fa_scores']\n",
    "    }[DATASET]\n",
    "    design_df = outputs[ds_key]['design']\n",
    "    best_idx  = outputs[ds_key]['best_iteration'].at[0, 'Original Index']\n",
    "    \n",
    "    # Extract features from the best iteration\n",
    "    feat_cols = [c for c in design_df.columns if c != 'Iteration']\n",
    "    selected  = [c for c in feat_cols if design_df.at[best_idx, c] == 1]\n",
    "    \n",
    "    X_top = df_used.drop(columns='y')[selected]\n",
    "    y_top = LabelEncoder().fit_transform(df_used['y'])\n",
    "    \n",
    "    grid_results = []\n",
    "    \n",
    "    # Loop over models\n",
    "    for model_name, model in MODELS.items():\n",
    "        # Before tuning (baseline CV metrics)\n",
    "        cvb = cross_validate(\n",
    "            model, X_top, y_top,\n",
    "            cv=cv, scoring=SCORING,\n",
    "            n_jobs=-1, return_train_score=False\n",
    "        )\n",
    "        before = {f\"{m}(Before)\": cvb[f\"test_{m}\"].mean() for m in SCORING}\n",
    "        \n",
    "        # Grid Search (optimize weighted F1)\n",
    "        grid = GridSearchCV(\n",
    "            model,\n",
    "            PARAM_GRIDS[model_name],\n",
    "            scoring='f1_weighted',\n",
    "            cv=cv,\n",
    "            n_jobs=-1,\n",
    "            refit=True\n",
    "        )\n",
    "        t0 = time.time()\n",
    "        grid.fit(X_top, y_top)\n",
    "        elapsed = time.time() - t0\n",
    "        best_params = grid.best_params_\n",
    "        \n",
    "        # After tuning (CV with best estimator)\n",
    "        tuned = grid.best_estimator_\n",
    "        cva   = cross_validate(\n",
    "            tuned, X_top, y_top,\n",
    "            cv=cv, scoring=SCORING,\n",
    "            n_jobs=-1, return_train_score=False\n",
    "        )\n",
    "        after = {f\"{m}(After)\": cva[f\"test_{m}\"].mean() for m in SCORING}\n",
    "        \n",
    "        # Compile row\n",
    "        row = {\n",
    "            'Model'          : model_name,\n",
    "            **before,\n",
    "            **after,\n",
    "            'Best Parameters': best_params,\n",
    "            'Time (s)'       : elapsed\n",
    "        }\n",
    "        grid_results.append(row)\n",
    "    \n",
    "    # Build DataFrame and store in outputs\n",
    "    df_gs = pd.DataFrame(grid_results).set_index('Model')\n",
    "    outputs[ds_key]['grid_results'] = df_gs\n",
    "\n",
    "# Persist Grid Search results to Excel\n",
    "with pd.ExcelWriter(\"df_grid_search.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    for i, (name, _) in enumerate(datasets.items(), start=1):\n",
    "        sheet = f\"{i}_{name}_{DATASET}\"[:31]\n",
    "        outputs[f\"dataset_{i}\"]['grid_results'].to_excel(writer, sheet_name=sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4370d0e0-166c-4105-99a7-29d9cc7f2611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets (RESULTS): 10 | Datasets (TIMES): 10\n",
      "[results] 10_waveform_database_generator\n",
      "[results] 1_statlog_australian_credit\n",
      "[results] 2_wine_quality_red\n",
      "[results] 3_breast_cancer_wisconsin\n",
      "[results] 4_burst_header_packet\n",
      "[results] 5_seismic_bumps\n",
      "[results] 6_iranian_churn\n",
      "[results] 7_qsar_biodegradation\n",
      "[results] 8_image_segmentation\n",
      "[results] 9_steel_plates_faults\n",
      "[times] 10_waveform_database_generator\n",
      "[times] 1_statlog_australian_credit\n",
      "[times] 2_wine_quality_red\n",
      "[times] 3_breast_cancer_wisconsin\n",
      "[times] 4_burst_header_packet\n",
      "[times] 5_seismic_bumps\n",
      "[times] 6_iranian_churn\n",
      "[times] 7_qsar_biodegradation\n",
      "[times] 8_image_segmentation\n",
      "[times] 9_steel_plates_faults\n",
      "\n",
      "=== Excel gerados ===\n",
      "- results_stats_summary.xlsx\n",
      "- results_posthoc_pairs.xlsx\n",
      "- times_stats_summary.xlsx\n",
      "- times_posthoc_pairs.xlsx\n",
      "\n",
      "Colunas novas: SW_p_* (Shapiro por grupo), Levene_p, median_diff ± CI, cliffs_delta ± CI.\n"
     ]
    }
   ],
   "source": [
    "# Statistical tests\n",
    "\n",
    "# Configuration\n",
    "PATHS = {\n",
    "    \"fa_results\":  \"fa_df_ml_results.xlsx\",\n",
    "    \"pca_results\": \"pca_df_ml_results.xlsx\",\n",
    "    \"orig_results\":\"original_df_ml_results.xlsx\",\n",
    "    \"fa_times\":    \"fa_df_ml_times.xlsx\",\n",
    "    \"pca_times\":   \"pca_df_ml_times.xlsx\",\n",
    "    \"orig_times\":  \"original_df_ml_times.xlsx\",\n",
    "}\n",
    "PREPROS     = [\"FA\", \"PCA\", \"Original\"]\n",
    "RESULT_KEYS = {\"FA\": \"fa_results\", \"PCA\": \"pca_results\", \"Original\": \"orig_results\"}\n",
    "TIME_KEYS   = {\"FA\": \"fa_times\",   \"PCA\": \"pca_times\",   \"Original\": \"orig_times\"}\n",
    "METRICS     = [\"A\", \"P\", \"R\", \"F1\"]\n",
    "PREF_SUFFIX = {\n",
    "    \"fa_results\":  [\"fa\"],\n",
    "    \"pca_results\": [\"pca\"],\n",
    "    \"orig_results\":[\"orig\", \"original\", \"org\"],\n",
    "    \"fa_times\":    [\"fa\"],\n",
    "    \"pca_times\":   [\"pca\"],\n",
    "    \"orig_times\":  [\"orig\", \"original\", \"org\"],\n",
    "}\n",
    "\n",
    "# Bootstrap setup\n",
    "N_BOOT   = 2000\n",
    "BOOT_SEED = 2025\n",
    "ALPHA     = 0.05\n",
    "\n",
    "# Worksheet helpers\n",
    "def sheet_prefix(name: str) -> str:\n",
    "    return name.rsplit(\"_\", 1)[0].strip() if \"_\" in name else name.strip()\n",
    "\n",
    "def sheet_suffix(name: str) -> str:\n",
    "    return name.rsplit(\"_\", 1)[1].strip().lower() if \"_\" in name else \"\"\n",
    "\n",
    "def choose_sheet_for_base(cands: list[str], prefer_suffixes: list[str] | None = None) -> str | None:\n",
    "    if not cands:\n",
    "        return None\n",
    "    if prefer_suffixes:\n",
    "        for pref in prefer_suffixes:\n",
    "            for c in cands:\n",
    "                if sheet_suffix(c) == pref.lower():\n",
    "                    return c\n",
    "    return cands[0]\n",
    "\n",
    "def read_sheet_by_base(book_path: Path, base: str, prefer_suffix: list[str] | None) -> pd.DataFrame:\n",
    "    xls = pd.ExcelFile(book_path)\n",
    "    cands = [s for s in xls.sheet_names if sheet_prefix(s) == base]\n",
    "    if not cands:\n",
    "        raise ValueError(f\"Sheet with prefix '{base}' not found in '{book_path.name}'. Sheets: {xls.sheet_names}\")\n",
    "    chosen = choose_sheet_for_base(cands, prefer_suffix)\n",
    "    return pd.read_excel(book_path, sheet_name=chosen)\n",
    "\n",
    "# Effect Sizes and Utilities\n",
    "def epsilon_squared_kw(H: float, k: int, N: int) -> float:\n",
    "    if N <= k:\n",
    "        return np.nan\n",
    "    return float(np.clip((H - k + 1) / (N - k), 0.0, 1.0))\n",
    "\n",
    "def cliffs_delta(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    x = np.asarray(x); y = np.asarray(y)\n",
    "    x = x[~np.isnan(x)]; y = y[~np.isnan(y)]\n",
    "    if x.size == 0 or y.size == 0:\n",
    "        return np.nan\n",
    "    greater = 0; less = 0\n",
    "    for xi in x:\n",
    "        greater += np.sum(xi > y)\n",
    "        less    += np.sum(xi < y)\n",
    "    nxy = x.size * y.size\n",
    "    return (greater - less) / nxy if nxy > 0 else np.nan\n",
    "\n",
    "def rank_biserial_from_mw(u_stat: float, n1: int, n2: int) -> float:\n",
    "    if n1 <= 0 or n2 <= 0:\n",
    "        return np.nan\n",
    "    return 1.0 - 2.0 * (u_stat / (n1 * n2))\n",
    "\n",
    "def holm_correction(pvals: list[float]) -> list[float]:\n",
    "    m = len(pvals)\n",
    "    order = np.argsort(pvals)\n",
    "    adj = np.empty(m, dtype=float)\n",
    "    prev = 0.0\n",
    "    for i, idx in enumerate(order):\n",
    "        adj_p = (m - i) * pvals[idx]\n",
    "        adj[idx] = max(adj_p, prev)\n",
    "        prev = adj[idx]\n",
    "    return np.minimum(adj, 1.0).tolist()\n",
    "\n",
    "# Diagnostics (normality/homoscedasticity)\n",
    "def shapiro_safe(x: np.ndarray) -> float:\n",
    "    \"\"\"Return Shapiro–Wilk p-value, or NaN if out of supported range (n<3 or n>5000).\"\"\"\n",
    "    x = np.asarray(x); x = x[~np.isnan(x)]\n",
    "    n = x.size\n",
    "    if n < 3 or n > 5000:\n",
    "        return np.nan\n",
    "    try:\n",
    "        return float(shapiro(x).pvalue)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def levene_safe(groups: list[np.ndarray], center: str = \"median\") -> float:\n",
    "    \"\"\"Return Levene p-value (robust) for 2+ groups; NaN if any empty.\"\"\"\n",
    "    cleaned = [np.asarray(g)[~np.isnan(g)] for g in groups]\n",
    "    if any(len(g) == 0 for g in cleaned) or len(cleaned) < 2:\n",
    "        return np.nan\n",
    "    try:\n",
    "        return float(levene(*cleaned, center=center).pvalue)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# Bootstrap CIs\n",
    "rng = np.random.default_rng(BOOT_SEED)\n",
    "\n",
    "def bootstrap_ci_diff_median(x: np.ndarray, y: np.ndarray, n_boot: int = N_BOOT, alpha: float = ALPHA) -> tuple[float,float,float]:\n",
    "    \"\"\"Return (median_diff, ci_low, ci_high).\"\"\"\n",
    "    x = np.asarray(x)[~np.isnan(x)]\n",
    "    y = np.asarray(y)[~np.isnan(y)]\n",
    "    if x.size == 0 or y.size == 0:\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "    diff = float(np.median(x) - np.median(y))\n",
    "    bx = rng.choice(x, size=(n_boot, x.size), replace=True)\n",
    "    by = rng.choice(y, size=(n_boot, y.size), replace=True)\n",
    "    diffs = np.median(bx, axis=1) - np.median(by, axis=1)\n",
    "    lo, hi = np.percentile(diffs, [100*alpha/2, 100*(1-alpha/2)])\n",
    "    return (diff, float(lo), float(hi))\n",
    "\n",
    "def bootstrap_ci_cliffs_delta(x: np.ndarray, y: np.ndarray, n_boot: int = N_BOOT, alpha: float = ALPHA) -> tuple[float,float,float]:\n",
    "    \"\"\"Return (delta, ci_low, ci_high).\"\"\"\n",
    "    x = np.asarray(x)[~np.isnan(x)]\n",
    "    y = np.asarray(y)[~np.isnan(y)]\n",
    "    if x.size == 0 or y.size == 0:\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "    base = cliffs_delta(x, y)\n",
    "    bx_idx = rng.integers(0, len(x), size=(n_boot, len(x)))\n",
    "    by_idx = rng.integers(0, len(y), size=(n_boot, len(y)))\n",
    "    deltas = []\n",
    "    for i in range(n_boot):\n",
    "        deltas.append(cliffs_delta(x[bx_idx[i]], y[by_idx[i]]))\n",
    "    lo, hi = np.percentile(deltas, [100*alpha/2, 100*(1-alpha/2)])\n",
    "    return (float(base), float(lo), float(hi))\n",
    "\n",
    "# Parsing and Aggregation\n",
    "def parse_algos_from_results_columns(columns: list[str]) -> dict[str, list[str]]:\n",
    "    pat = re.compile(r\"^(?P<algo>.+)\\((?P<m>A|P|R|F1)\\)$\", flags=re.I)\n",
    "    temp = {}\n",
    "    for col in columns:\n",
    "        m = pat.match(str(col).strip())\n",
    "        if not m:\n",
    "            continue\n",
    "        algo = m.group(\"algo\").strip()\n",
    "        met  = m.group(\"m\").upper()\n",
    "        temp.setdefault(algo, {})[met] = col\n",
    "    algo2cols = {}\n",
    "    for algo, d in temp.items():\n",
    "        if all(m in d for m in METRICS):\n",
    "            algo2cols[algo] = [d[m] for m in METRICS]\n",
    "    return algo2cols\n",
    "\n",
    "def average_metrics_per_algo(df: pd.DataFrame, algo2cols: dict[str, list[str]]) -> pd.DataFrame:\n",
    "    out = {}\n",
    "    for algo, cols in algo2cols.items():\n",
    "        out[algo] = df[cols].astype(float).mean(axis=1, skipna=True)\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "def extract_times_wide(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    time_cols = [c for c in df.columns if str(c).endswith(\"_time\")]\n",
    "    if not time_cols:\n",
    "        time_cols = [c for c in df.columns if \"time\" in str(c).lower() or \"tempo\" in str(c).lower()]\n",
    "    rename = {c: re.sub(r\"_time$\", \"\", str(c)) for c in time_cols}\n",
    "    return df[time_cols].astype(float).rename(columns=rename)\n",
    "\n",
    "# Tests (KW, MW, and Diagnostics)\n",
    "def kw_and_diagnostics(groups: dict[str, np.ndarray]) -> dict:\n",
    "    \"\"\"Kruskal–Wallis + epsilon² + diagnostics (per-group Shapiro; Levene).\"\"\"\n",
    "    labels = list(groups.keys())\n",
    "    data = [np.asarray(groups[l], dtype=float) for l in labels]\n",
    "    data = [d[~np.isnan(d)] for d in data]\n",
    "    sizes = [len(d) for d in data]\n",
    "    N = sum(sizes); k = len(labels)\n",
    "\n",
    "    res = {\"KW_H\": np.nan, \"KW_p\": np.nan, \"KW_k\": k, \"KW_N\": N, \"KW_epsilon_sq\": np.nan}\n",
    "    if any(s == 0 for s in sizes) or k < 2:\n",
    "        # Still compute diagnostics\n",
    "        for lab, arr in zip(labels, data):\n",
    "            res[f\"SW_p_{lab}\"] = shapiro_safe(arr)\n",
    "        res[\"Levene_p\"] = levene_safe(data, center=\"median\")\n",
    "        return res\n",
    "\n",
    "    H, p_kw = kruskal(*data, nan_policy=\"omit\")\n",
    "    eps2 = epsilon_squared_kw(H, k, N)\n",
    "    res.update({\"KW_H\": H, \"KW_p\": p_kw, \"KW_epsilon_sq\": eps2})\n",
    "\n",
    "    # Diagnostics\n",
    "    for lab, arr in zip(labels, data):\n",
    "        res[f\"SW_p_{lab}\"] = shapiro_safe(arr)\n",
    "    res[\"Levene_p\"] = levene_safe(data, center=\"median\")\n",
    "    return res\n",
    "\n",
    "def mw_posthoc_with_boot(groups: dict[str, np.ndarray]) -> pd.DataFrame:\n",
    "    \"\"\"Mann–Whitney U + effect sizes + CIs (median diff & Cliff's delta) + Holm.\"\"\"\n",
    "    labels = list(groups.keys())\n",
    "    data = {l: np.asarray(groups[l], dtype=float)[~np.isnan(groups[l])] for l in labels}\n",
    "\n",
    "    pairs = list(itertools.combinations(labels, 2))\n",
    "    rows = []\n",
    "    for a, b in pairs:\n",
    "        x, y = data[a], data[b]\n",
    "        if len(x) == 0 or len(y) == 0:\n",
    "            rows.append({\n",
    "                \"pair\": f\"{a} vs {b}\", \"U\": np.nan, \"p\": np.nan, \"p_adj_holm\": np.nan,\n",
    "                \"cliffs_delta\": np.nan, \"cliffs_delta_ci_low\": np.nan, \"cliffs_delta_ci_high\": np.nan,\n",
    "                \"rank_biserial\": np.nan,\n",
    "                \"median_diff\": np.nan, \"median_diff_ci_low\": np.nan, \"median_diff_ci_high\": np.nan,\n",
    "                \"direction\": \"\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        mw = mannwhitneyu(x, y, alternative=\"two-sided\")\n",
    "        u_stat, pval = mw.statistic, mw.pvalue\n",
    "        delta, d_lo, d_hi = bootstrap_ci_cliffs_delta(x, y, n_boot=N_BOOT, alpha=ALPHA)\n",
    "        rb = rank_biserial_from_mw(u_stat, len(x), len(y))\n",
    "        mdiff, m_lo, m_hi = bootstrap_ci_diff_median(x, y, n_boot=N_BOOT, alpha=ALPHA)\n",
    "        direction = f\"{a}>{b}\" if np.nanmedian(x) > np.nanmedian(y) else f\"{b}>{a}\"\n",
    "\n",
    "        rows.append({\n",
    "            \"pair\": f\"{a} vs {b}\",\n",
    "            \"U\": u_stat, \"p\": pval,\n",
    "            \"cliffs_delta\": delta, \"cliffs_delta_ci_low\": d_lo, \"cliffs_delta_ci_high\": d_hi,\n",
    "            \"rank_biserial\": rb,\n",
    "            \"median_diff\": mdiff, \"median_diff_ci_low\": m_lo, \"median_diff_ci_high\": m_hi,\n",
    "            \"direction\": direction\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if not df.empty:\n",
    "        df[\"p_adj_holm\"] = holm_correction(df[\"p\"].fillna(1.0).tolist())\n",
    "    return df\n",
    "\n",
    "# Analyses: Performance and Time\n",
    "def analyze_results_for_base(book_paths: dict[str, Path], base: str) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    dfs = {}\n",
    "    algo_sets = []\n",
    "    for label, key in [(\"FA\", \"fa_results\"), (\"PCA\", \"pca_results\"), (\"Original\", \"orig_results\")]:\n",
    "        df = read_sheet_by_base(book_paths[key], base, PREF_SUFFIX.get(key))\n",
    "        algo2cols = parse_algos_from_results_columns(list(df.columns))\n",
    "        algo_sets.append(set(algo2cols.keys()))\n",
    "        dfs[label] = average_metrics_per_algo(df, algo2cols)\n",
    "\n",
    "    common_algos = set.intersection(*algo_sets) if algo_sets else set()\n",
    "    stats_rows = []\n",
    "    pairs_all = []\n",
    "\n",
    "    for algo in sorted(common_algos):\n",
    "        groups = {pre: dfs[pre][algo].values for pre in PREPROS}\n",
    "        # KW + diagnostics\n",
    "        kw_diag = kw_and_diagnostics(groups)\n",
    "        stats_rows.append({\n",
    "            \"dataset_base\": base, \"section\": \"results\", \"level\": \"per-algorithm\", \"target\": algo, **kw_diag\n",
    "        })\n",
    "        # Post-hoc + bootstrap CIs\n",
    "        df_pairs = mw_posthoc_with_boot(groups)\n",
    "        if not df_pairs.empty:\n",
    "            df_pairs.insert(0, \"dataset_base\", base)\n",
    "            df_pairs.insert(1, \"section\", \"results\")\n",
    "            df_pairs.insert(2, \"level\", \"per-algorithm\")\n",
    "            df_pairs.insert(3, \"target\", algo)\n",
    "            pairs_all.append(df_pairs)\n",
    "\n",
    "    # Overview (mean across common algorithms)\n",
    "    if common_algos:\n",
    "        mean_by_pre = {}\n",
    "        for pre in PREPROS:\n",
    "            mean_by_pre[pre] = dfs[pre][sorted(common_algos)].mean(axis=1, skipna=True).values\n",
    "        kw_diag = kw_and_diagnostics(mean_by_pre)\n",
    "        stats_rows.append({\n",
    "            \"dataset_base\": base, \"section\": \"results\", \"level\": \"overview\", \"target\": \"MEAN_OVER_ALGOS\", **kw_diag\n",
    "        })\n",
    "        df_pairs = mw_posthoc_with_boot(mean_by_pre)\n",
    "        if not df_pairs.empty:\n",
    "            df_pairs.insert(0, \"dataset_base\", base)\n",
    "            df_pairs.insert(1, \"section\", \"results\")\n",
    "            df_pairs.insert(2, \"level\", \"overview\")\n",
    "            df_pairs.insert(3, \"target\", \"MEAN_OVER_ALGOS\")\n",
    "            pairs_all.append(df_pairs)\n",
    "\n",
    "    df_stats = pd.DataFrame(stats_rows)\n",
    "    df_pairs = pd.concat(pairs_all, ignore_index=True, sort=False) if pairs_all else pd.DataFrame()\n",
    "    return df_stats, df_pairs\n",
    "\n",
    "def analyze_times_for_base(book_paths: dict[str, Path], base: str) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    dfs = {}\n",
    "    algo_sets = []\n",
    "    for label, key in [(\"FA\", \"fa_times\"), (\"PCA\", \"pca_times\"), (\"Original\", \"orig_times\")]:\n",
    "        df = read_sheet_by_base(book_paths[key], base, PREF_SUFFIX.get(key))\n",
    "        tdf = extract_times_wide(df)\n",
    "        algo_sets.append(set(tdf.columns))\n",
    "        dfs[label] = tdf\n",
    "\n",
    "    common_algos = set.intersection(*algo_sets) if algo_sets else set()\n",
    "    stats_rows = []\n",
    "    pairs_all = []\n",
    "\n",
    "    for algo in sorted(common_algos):\n",
    "        groups = {pre: dfs[pre][algo].values for pre in PREPROS}\n",
    "        kw_diag = kw_and_diagnostics(groups)\n",
    "        stats_rows.append({\n",
    "            \"dataset_base\": base, \"section\": \"times\", \"level\": \"per-algorithm_time\", \"target\": algo, **kw_diag\n",
    "        })\n",
    "        df_pairs = mw_posthoc_with_boot(groups)\n",
    "        if not df_pairs.empty:\n",
    "            df_pairs.insert(0, \"dataset_base\", base)\n",
    "            df_pairs.insert(1, \"section\", \"times\")\n",
    "            df_pairs.insert(2, \"level\", \"per-algorithm_time\")\n",
    "            df_pairs.insert(3, \"target\", algo)\n",
    "            pairs_all.append(df_pairs)\n",
    "\n",
    "    if common_algos:\n",
    "        mean_by_pre = {}\n",
    "        for pre in PREPROS:\n",
    "            mean_by_pre[pre] = dfs[pre][sorted(common_algos)].mean(axis=1, skipna=True).values\n",
    "        kw_diag = kw_and_diagnostics(mean_by_pre)\n",
    "        stats_rows.append({\n",
    "            \"dataset_base\": base, \"section\": \"times\", \"level\": \"overview_time\", \"target\": \"MEAN_OVER_ALGOS_TIME\", **kw_diag\n",
    "        })\n",
    "        df_pairs = mw_posthoc_with_boot(mean_by_pre)\n",
    "        if not df_pairs.empty:\n",
    "            df_pairs.insert(0, \"dataset_base\", base)\n",
    "            df_pairs.insert(1, \"section\", \"times\")\n",
    "            df_pairs.insert(2, \"level\", \"overview_time\")\n",
    "            df_pairs.insert(3, \"target\", \"MEAN_OVER_ALGOS_TIME\")\n",
    "            pairs_all.append(df_pairs)\n",
    "\n",
    "    df_stats = pd.DataFrame(stats_rows)\n",
    "    df_pairs = pd.concat(pairs_all, ignore_index=True, sort=False) if pairs_all else pd.DataFrame()\n",
    "    return df_stats, df_pairs\n",
    "\n",
    "# Save to Excel (One sheet per dataset)\n",
    "def safe_sheet_name(name: str, fallback: str = \"sheet\") -> str:\n",
    "    bad = r'[:\\\\/?*\\[\\]]'\n",
    "    name = re.sub(bad, \"_\", str(name)).strip().rstrip(\".\")\n",
    "    return (name or fallback)[:31]\n",
    "\n",
    "def run_all_to_excel():\n",
    "    base_dir = Path(\".\")\n",
    "    book_paths = {k: base_dir / v for k, v in PATHS.items()}\n",
    "    for k, p in book_paths.items():\n",
    "        if not p.exists():\n",
    "            print(f\"[WARNING] File not found: {p.resolve()} (key: {k})\")\n",
    "\n",
    "    def collect_bases(keys):\n",
    "        bases_sets = []\n",
    "        for key in keys:\n",
    "            xls = pd.ExcelFile(book_paths[key])\n",
    "            bases_sets.append({sheet_prefix(s) for s in xls.sheet_names})\n",
    "        return sorted(list(set.intersection(*bases_sets))) if bases_sets else []\n",
    "\n",
    "    bases_results = collect_bases([\"fa_results\",\"pca_results\",\"orig_results\"])\n",
    "    bases_times   = collect_bases([\"fa_times\",\"pca_times\",\"orig_times\"])\n",
    "\n",
    "    print(f\"Datasets (RESULTS): {len(bases_results)} | Datasets (TIMES): {len(bases_times)}\")\n",
    "\n",
    "    res_stats_by_base, res_pairs_by_base = {}, {}\n",
    "    tim_stats_by_base, tim_pairs_by_base = {}, {}\n",
    "\n",
    "    for base in bases_results:\n",
    "        print(f\"[results] {base}\")\n",
    "        res_stats, res_pairs = analyze_results_for_base(book_paths, base)\n",
    "        res_stats_by_base[base] = res_stats\n",
    "        res_pairs_by_base[base] = res_pairs\n",
    "\n",
    "    for base in bases_times:\n",
    "        print(f\"[times] {base}\")\n",
    "        t_stats, t_pairs = analyze_times_for_base(book_paths, base)\n",
    "        tim_stats_by_base[base] = t_stats\n",
    "        tim_pairs_by_base[base] = t_pairs\n",
    "\n",
    "    # results_stats_summary.xlsx (Includes Shapiro and Levene)\n",
    "    with pd.ExcelWriter(\"results_stats_summary.xlsx\", engine=\"openpyxl\") as writer:\n",
    "        for base, df in res_stats_by_base.items():\n",
    "            diag_cols = [c for c in df.columns if c.startswith(\"SW_p_\")] + ([\"Levene_p\"] if \"Levene_p\" in df.columns else [])\n",
    "            core = [\"dataset_base\",\"section\",\"level\",\"target\",\"KW_H\",\"KW_p\",\"KW_k\",\"KW_N\",\"KW_epsilon_sq\"]\n",
    "            cols_order = [c for c in core if c in df.columns] + diag_cols + [c for c in df.columns if c not in core + diag_cols]\n",
    "            df[cols_order].to_excel(writer, sheet_name=safe_sheet_name(base), index=False)\n",
    "\n",
    "    # results_posthoc_pairs.xlsx (With CIs)\n",
    "    with pd.ExcelWriter(\"results_posthoc_pairs.xlsx\", engine=\"openpyxl\") as writer:\n",
    "        for base, df in res_pairs_by_base.items():\n",
    "            core = [\"dataset_base\",\"section\",\"level\",\"target\",\"pair\",\"U\",\"p\",\"p_adj_holm\",\n",
    "                    \"cliffs_delta\",\"cliffs_delta_ci_low\",\"cliffs_delta_ci_high\",\n",
    "                    \"rank_biserial\",\n",
    "                    \"median_diff\",\"median_diff_ci_low\",\"median_diff_ci_high\",\n",
    "                    \"direction\"]\n",
    "            cols_order = [c for c in core if c in df.columns] + [c for c in df.columns if c not in core]\n",
    "            df[cols_order].to_excel(writer, sheet_name=safe_sheet_name(base), index=False)\n",
    "\n",
    "    # times_stats_summary.xlsx (Includes Shapiro and Levene)\n",
    "    with pd.ExcelWriter(\"times_stats_summary.xlsx\", engine=\"openpyxl\") as writer:\n",
    "        for base, df in tim_stats_by_base.items():\n",
    "            diag_cols = [c for c in df.columns if c.startswith(\"SW_p_\")] + ([\"Levene_p\"] if \"Levene_p\" in df.columns else [])\n",
    "            core = [\"dataset_base\",\"section\",\"level\",\"target\",\"KW_H\",\"KW_p\",\"KW_k\",\"KW_N\",\"KW_epsilon_sq\"]\n",
    "            cols_order = [c for c in core if c in df.columns] + diag_cols + [c for c in df.columns if c not in core + diag_cols]\n",
    "            df[cols_order].to_excel(writer, sheet_name=safe_sheet_name(base), index=False)\n",
    "\n",
    "    # times_posthoc_pairs.xlsx (With CIs)\n",
    "    with pd.ExcelWriter(\"times_posthoc_pairs.xlsx\", engine=\"openpyxl\") as writer:\n",
    "        for base, df in tim_pairs_by_base.items():\n",
    "            core = [\"dataset_base\",\"section\",\"level\",\"target\",\"pair\",\"U\",\"p\",\"p_adj_holm\",\n",
    "                    \"cliffs_delta\",\"cliffs_delta_ci_low\",\"cliffs_delta_ci_high\",\n",
    "                    \"rank_biserial\",\n",
    "                    \"median_diff\",\"median_diff_ci_low\",\"median_diff_ci_high\",\n",
    "                    \"direction\"]\n",
    "            cols_order = [c for c in core if c in df.columns] + [c for c in df.columns if c not in core]\n",
    "            df[cols_order].to_excel(writer, sheet_name=safe_sheet_name(base), index=False)\n",
    "\n",
    "    print(\"\\n=== Generated Excel files ===\")\n",
    "    print(\"- results_stats_summary.xlsx\")\n",
    "    print(\"- results_posthoc_pairs.xlsx\")\n",
    "    print(\"- times_stats_summary.xlsx\")\n",
    "    print(\"- times_posthoc_pairs.xlsx\")\n",
    "    print(\"\\nNew columns: SW_p_* (Shapiro per group), Levene_p, median_diff ± CI, cliffs_delta ± CI.\")\n",
    "\n",
    "# Run\n",
    "run_all_to_excel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
